{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tonkenization\n",
    "\n",
    "#### Definition\n",
    "- __Tonkenization__ or __word segmentation__ is the process of separating (or tokenizing) words in raw text.\n",
    "- First pre-processing step in NLP applications.\n",
    "- Nearly all NLP and ML libraries and toolkits include highly optimized tokenizers. So, in practice, writing a new tokenizer is not necessary.\n",
    "- In this notebook I will implement a simple tokenization algorithm based on regular expressions and compare it to one of the available ones from the NLP Toolkit.\n",
    "\n",
    "#### Challenges\n",
    "- How would we define a word? - White spaces might not be relevant in all cases.\n",
    "    - Sometimes we would like to keep some words together as a single unit, because they only hold a meaning in context.\n",
    "    - Not all languages define words by whitespaces.\n",
    "    - Separating by puntuation marks may not be generalizable: what about apostrophes, acronyms, or number expressions?\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import data\n",
    "\n",
    "We can import data from NLP Toolkit or NLTK (see how to import data from the web https://www.nltk.org/book_1ed/ch02.html)\n",
    "\n",
    "We will extract the first sentence of the chosen text as an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['firefox.txt',\n",
       " 'grail.txt',\n",
       " 'overheard.txt',\n",
       " 'pirates.txt',\n",
       " 'singles.txt',\n",
       " 'wine.txt']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re, nltk\n",
    "from nltk.corpus import webtext\n",
    "\n",
    "#nltk.download('webtext')\n",
    "\n",
    "# show list of .txt files in the corpus\n",
    "webtext.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download pirates.txt for the webtext corpus\n",
    "\n",
    "full_text = webtext.raw(\"pirates.txt\")\n",
    "#print(full_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PIRATES OF THE CARRIBEAN: DEAD MAN'S CHEST, by Ted Elliott & Terry Rossio\n"
     ]
    }
   ],
   "source": [
    "# extract first sentence\n",
    "sentence = full_text.split(\"\\n\")[0]\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple tonkenization algorithm\n",
    "\n",
    "The most obvious way of separating words is splitting them by whitespaces:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PIRATES', 'OF', 'THE', 'CARRIBEAN:', 'DEAD', \"MAN'S\", 'CHEST,', 'by', 'Ted', 'Elliott', '&', 'Terry', 'Rossio']\n"
     ]
    }
   ],
   "source": [
    "# re.split(\" \", str) splits the string based on this characters defined on the first argument\n",
    "\n",
    "print(sentence.split(\" \"))\n",
    "\n",
    "# re.split(\"\\s+\", str) splits a string based on the regular expression \\s+.\n",
    "# the regex expression \\s+ returns a match where the string contains a white space character.\n",
    "\n",
    "#print(re.split(\"\\s+\", sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This approach leaves the punctuation marks on the tonkenized strings and is not generalizable.\n",
    "\n",
    "Let's try to split by punctuation marks too using pattern-matching instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PIRATES', 'OF', 'THE', 'CARRIBEAN', 'DEAD', 'MAN', 'S', 'CHEST', 'by', 'Ted', 'Elliott', '&', 'Terry', 'Rossio']\n"
     ]
    }
   ],
   "source": [
    "tokens = re.split('[(\\s+.,:;!?\\'\\\")]', sentence)\n",
    "#print(tokens)\n",
    "\n",
    "tokenized = [x for x in tokens if x != '' and x not in '- \\t\\n.,;:!?[]']\n",
    "print(tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This seems to work well. However, when applied to the sentece below it fails to keep the \"\" which can give the phrase very positive a very important subjective tone. \n",
    "\n",
    "Also, it takes away the apostrophe at the end of Containers', which changes the meaning of the sentence. \n",
    "\n",
    "This simple tonkekization algorithm cannot handle abbreviations and therefore ignores the \".\" after \"Mr.\", which makes it lose some meaning.\n",
    "\n",
    "\"Mr. Sherwood said reaction to Sea Containers' proposal has been \\\"very positive.\\\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Mr', 'Sherwood', 'said', 'reaction', 'to', 'Sea', 'Containers', 'proposal', 'has', 'been', 'very', 'positive']\n"
     ]
    }
   ],
   "source": [
    "sentence_ = \"Mr. Sherwood said reaction to Sea Containers' proposal has been \\\"very positive.\\\"\"\n",
    "\n",
    "tokens_ = re.split('[(\\s+.,:;!?\\'\\\")]', sentence_)\n",
    "#print(tokens_)\n",
    "\n",
    "tokenized_ = [x for x in tokens_ if x != '' and x not in '- \\t\\n.,;:!?[]']\n",
    "print(tokenized_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply NLTK's tokenizer\n",
    "\n",
    "See https://www.nltk.org/book/ch03.html\n",
    "\n",
    "To solve the problem with our algorithm not being able to recognize abbreviations, we can import __punkt__, a tonkenizer algorithm that splits a text into sentences while handling special cases like abbreviations, collocations and words that start sentences (see nltk.org/_modules/nltk/tokenize/punkt.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PIRATES', 'OF', 'THE', 'CARRIBEAN', ':', 'DEAD', 'MAN', \"'S\", 'CHEST', ',', 'by', 'Ted', 'Elliott', '&', 'Terry', 'Rossio']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\win10\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "\n",
    "tokens = word_tokenize(sentence)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### More pre-processing: standardize and sort all the data\n",
    "\n",
    "Let's convert all words to lower case and sort them in alphabetical order forming our vocabulary with all the word ocurrences we will be working with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['pirates', 'of', 'the', 'carribean', ':', 'dead', 'man', \"'s\", 'chest', ',', 'by', 'ted', 'elliott', '&', 'terry', 'rossio']\n",
      "['&', \"'s\", ',', ':', 'by', 'carribean', 'chest', 'dead', 'elliott', 'man', 'of', 'pirates', 'rossio', 'ted', 'terry', 'the']\n"
     ]
    }
   ],
   "source": [
    "words = [w.lower() for w in tokens]\n",
    "print(words)\n",
    "\n",
    "vocab = sorted(set(words))\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While I could keep developing my own tokeniser to accommodate more cases, I will rely on the tokenization algorithms available on NLP toolkits. These are highly optimised and keep track of those instances in which words shouldn't be separated like in the following examples taken from \"Getting Started with Natural Language Processing\" by Ekaterina Kochmar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['What', \"'s\", 'the', 'best', 'way', 'to', 'cook', 'a', 'pizza', '?']\n",
      "['We', \"'re\", 'going', 'to', 'use', 'a', 'baking', 'stone']\n",
      "['I', 'have', \"n't\", 'used', 'a', 'baking', 'stone', 'before']\n"
     ]
    }
   ],
   "source": [
    "text1 = \"What's the best way to cook a pizza?\"\n",
    "text2 = \"We're going to use a baking stone\"\n",
    "text3 = \"I haven't used a baking stone before\"\n",
    "\n",
    "\n",
    "print(word_tokenize(text1))\n",
    "print(word_tokenize(text2))\n",
    "print(word_tokenize(text3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frequency Analysis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
